# study_oml
> [OML](https://open-metric-learning.readthedocs.io/en/latest/index.html)

>> [Training](https://colab.research.google.com/drive/1kntDAIdIZ9L40jcndguLAb-XqmCFOgS5?usp=sharing)
```
# https://open-metric-learning.readthedocs.io/en/latest/contents/utils.html#download-mock-dataset
dataset_root = "mock_dataset/"
df_train, _ = download_mock_dataset(dataset_root)
df_train.head()
```
![image](https://github.com/Suyeon-j/study_oml/assets/66247203/ae6c2e91-b44a-469c-bd63-32bcecc8456d)
Return: train과 validation Dataframe
>>> 구조 (8x3)


![image](https://github.com/Suyeon-j/study_oml/assets/66247203/ba0849ad-4d36-42db-96ad-615fea39551f)


```
# https://open-metric-learning.readthedocs.io/en/latest/contents/models.html#vitextractor
# 03
model = ViTExtractor("vits16_dino", arch="vits16", normalise_features=False).train()
model
```
ViTExtractor(weights(사전 훈련된 가중치 지정 매개변수), arch(모델 아키텍처 선택), normalise_features(특성 정규화 여부)): [Visual Transformer 아키텍처](https://gaussian37.github.io/dl-concept-vit/)를 따르는 추출기의 기본 클래스
![image](https://github.com/Suyeon-j/study_oml/assets/66247203/99d36ddd-809c-4933-93ef-530d90d028e3)

```
# https://pytorch.org/docs/stable/generated/torch.optim.SGD.html
# 05
optimizer = torch.optim.SGD(model.parameters(), lr=1e-6)
optimizer
```
![image](https://github.com/Suyeon-j/study_oml/assets/66247203/3673524f-5ec8-43b2-abee-05368a180674)

```
# https://open-metric-learning.readthedocs.io/en/latest/contents/datasets.html#datasetwithlabels
# 01
train_dataset = DatasetWithLabels(df_train, dataset_root=dataset_root)
train_dataset.df
```
![image](https://github.com/Suyeon-j/study_oml/assets/66247203/2d180786-09ff-47c6-ab2e-fb91c476c8dd)


```
# https://open-metric-learning.readthedocs.io/en/latest/contents/losses.html#tripletlosswithminer
# 04
criterion = TripletLossWithMiner(margin=0.1, miner=AllTripletsMiner(), need_logs=True)
criterion
```
TripletLossWithMiner(margin(triplet loss의 마진 설정), miner(triplet을 생성하는데 사용되는 miner를 지정하는 함수), need_logs(로그 기록 여부)): Miner and TripletLoss 조합

```
# https://open-metric-learning.readthedocs.io/en/latest/contents/samplers.html#balancesampler
# 02-01
sampler = BalanceSampler(train_dataset.get_labels(), n_labels=2, n_instances=2)
print(sampler.lbl2idx) # 라벨과 그에 해당하는 인덱스 반환
print(sampler.batch_size)
```
![image](https://github.com/Suyeon-j/study_oml/assets/66247203/5ebb9660-9554-41f3-b92c-daeac3529851)

BalanceSampler(labels, n_labels, n_instances): n_instances x n_labels 형태 배치 생성

```
# https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader
# 02-02
train_loader = torch.utils.data.DataLoader(train_dataset, batch_sampler=sampler)
train_loader
```
DataLoader(): 데이터셋을 읽어와서 배치단위로 데이터 불러옴

```
for batch in tqdm(train_loader):
    embeddings = model(batch["input_tensors"])
    loss = criterion(embeddings, batch["labels"])
    print("\n",batch["input_tensors"], "\n", batch["labels"])
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()

    # info for logging: positive/negative distances, number of active triplets
    print(criterion.last_logs)
```
![image](https://github.com/Suyeon-j/study_oml/assets/66247203/4a7e6f47-8531-482e-9e98-a7e26c38ee74)


active_tri: 활성화된 Triplets(Triplet Loss 함수에서 실제로 모델이 학습하고 있는 경우)의 비율을 나타내는 지표

높을수록 유효한 정보를 뽑아냈다고 할 수 있음

>> [Validation](https://colab.research.google.com/drive/1O2o3k8I8jN5hRin3dKnAS3WsgG04tmIT?usp=sharing)
```
# https://open-metric-learning.readthedocs.io/en/latest/contents/utils.html#download-mock-dataset
dataset_root =  "mock_dataset/"
_, df_val = download_mock_dataset(dataset_root)
df_val.head()
```
![image](https://github.com/Suyeon-j/study_oml/assets/66247203/5c0a1608-5e68-4860-85ca-65132e4fec82)
구조: training과 동일

```
# https://open-metric-learning.readthedocs.io/en/latest/contents/models.html#vitextractor
model = ViTExtractor("vits16_dino", arch="vits16", normalise_features=False).eval()
model
```
![image](https://github.com/Suyeon-j/study_oml/assets/66247203/1474294c-6c5d-47f7-9390-02d6dc48b411)

```
# https://open-metric-learning.readthedocs.io/en/latest/contents/datasets.html#datasetquerygallery
val_dataset = DatasetQueryGallery(df_val, dataset_root=dataset_root)
val_dataset.df
```
DatasetQueryGallery(df, dataset_root): 쿼리와 갤러리 분할 정보 제공
![image](https://github.com/Suyeon-j/study_oml/assets/66247203/d8e6c3ee-b6d5-489e-8dd4-b0bedfc17e77)

```
# https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader
val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=4)
```
training과 달리 손실함수(triplet)을 사용하지 않기 때문에 데이터 샘플링을 할 필요가 없음 -> batch_sampler 대신 batch_size 사용

DatasetQueryGallery(): 검증 단계에서 데이터 세트로 사용하기 위해 쿼리/갤러리 분할 정보를 넘겨줘야함 +) 유효성 검사 프로세스를 수행할 때 데이터셋이 분할 정보를 포함하고 있지않다면 is_query와 is_gallery가 모두 True여야 함

```
# https://open-metric-learning.readthedocs.io/en/latest/contents/metrics.html#embeddingmetrics
calculator = EmbeddingMetrics(extra_keys=["paths",])
calculator.setup(num_samples=len(val_dataset))
```
EmbeddingMetrics(extra_keys(배치에서 몇가지 정보를 추가 정보를 축적하는 키)): 매 배치마다 모델이 생성한 배치와 임베딩에서 얻은 정보를 에포크 단위로 축적

calculator.setup(num_samples=len(val_dataset)): self.update_data() 첫 호출 전에 호출해야 함

```
# https://open-metric-learning.readthedocs.io/en/latest/contents/metrics.html#oml.metrics.embeddings.EmbeddingMetrics.update_data
with torch.no_grad():
    for batch in tqdm(val_loader):
        batch["embeddings"] = model(batch["input_tensors"])
        calculator.update_data(batch)
```
calculator.update_data(): 나중에 메트릭을 계산하기 위해 데이터를 전달하는 메소드

![image](https://github.com/Suyeon-j/study_oml/assets/66247203/1bc6a59c-3616-46c3-95d3-bb882e0b04a2)


```
# Logging
print(calculator.metrics)  # metrics
print(calculator.metrics_unreduced)  # metrics without averaging over queries(쿼리에 대한 평균을 사용하지 않음)
```
![image](https://github.com/Suyeon-j/study_oml/assets/66247203/092099ff-d1d6-4c47-ae00-28dd9ebf4a7c)
cmc: 쿼리까지의 거리로 정렬된 상위 k 갤러리 인스턴스에 이 쿼리와 관련된 인스턴스가 하나 이상 있는 경우 1, 그렇지 않은 경우 0 (cmc@k: 각 쿼리에 대해 계산된 결과 평균)
map: 평균 정밀도를 계산하는 함수(map@k: recall의 함수로 간주되는 정밀도의 평균 값)
pcf: 주성분 분석을 사용해 임베딩의 주성분 비율을 추정(metric은 데이터의 필요한 분산을 설명하는데 필요한 성분의 비율로 정의)
precision: 정밀도(precision@k: 쿼리에서 갤러리까지의 거리로 정렬된 상위 k개 인스턴스 중 관련 갤러리 인스턴스의 일부)

```
import matplotlib.pyplot as plt

# Visualisation
# https://open-metric-learning.readthedocs.io/en/latest/contents/metrics.html#oml.metrics.embeddings.EmbeddingMetrics.get_plot_for_queries
print("Draw predictions for predefined queries")
calculator.get_plot_for_queries(query_ids=[2, 1], n_instances=5, verbose=False)
plt.show()
```
calculator.get_plot_for_queries(query_ids, n_instances, verbose): <query_ids> 표시를 사용하여 쿼리에 대한 예측을 시각화

query_ids, 쿼리의 인덱스 리스트

n_instances, 예측할 수

![image](https://github.com/Suyeon-j/study_oml/assets/66247203/e9f3926f-d019-44af-bde9-52b87c27d754)

+) verbose=True
![image](https://github.com/Suyeon-j/study_oml/assets/66247203/3dad42f3-a682-4f1e-99b9-34ddaffccb46)

```
print("Draw the queries worst by map@5")
calculator.get_plot_for_worst_queries(metric_name="OVERALL/map/5", n_queries=2, n_instances=5, verbose=False)
plt.show()
```
calculator.get_plot_for_worst_queries(): map@5로 최악의 쿼리 그리기

![image](https://github.com/Suyeon-j/study_oml/assets/66247203/080d40f3-bba6-4bc6-a1a5-62e2254cf8f9)

+)
```
calculator.get_plot_for_queries(query_ids=[0,1,2,3], n_instances=8, verbose=False)
```
![image](https://github.com/Suyeon-j/study_oml/assets/66247203/b5a03950-6a9c-42f2-b231-4d3979e11ceb)

12개의 이미지를 4개의 쿼리와 8개의 인스턴스 2개의 검증이미지

쿼리 4개를 뺀 나머지 이미지는 갤러리이다.

![image](https://github.com/Suyeon-j/study_oml/assets/66247203/655cace8-9fff-4405-bdeb-3f0999f7b0d4)


>> [Training + Validation [Lightning and logging]](https://colab.research.google.com/drive/1bVUgdBGWvQgCkba2YtaIRVlUQUz7Q60Z?usp=share_link)
```
# https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.ioff.html
plt.ioff()
```
plt.ioff(): 대화모드 비활성화

```
# train
optimizer = torch.optim.SGD(model.parameters(), lr=1e-6) # 04
train_dataset = DatasetWithLabels(df_train, dataset_root=dataset_root) # 01
criterion = TripletLossWithMiner(margin=0.1, miner=AllTripletsMiner()) # 05
batch_sampler = BalanceSampler(train_dataset.get_labels(), n_labels=2, n_instances=3) # 03
train_loader = torch.utils.data.DataLoader(train_dataset, batch_sampler=batch_sampler) # 02
```
질문1) 인스턴스 개수 훈련과 다르게 설정한 이유

```
# val
# https://open-metric-learning.readthedocs.io/en/latest/contents/lightning.html#metricvalcallback
val_dataset = DatasetQueryGallery(df_val, dataset_root=dataset_root)
val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=4)
metric_callback = MetricValCallback(metric=EmbeddingMetrics(extra_keys=["paths",]), log_images=True);
```
// 라이트닝 함수: https://open-metric-learning.readthedocs.io/en/latest/_modules/oml/lightning/callbacks/metric.html#MetricValCallback


MetricValCallback(metric, log_images): 파이토치 라이트닝과 함께 IBasic Metric을 사용할 수 있는 함수


// https://open-metric-learning.readthedocs.io/en/latest/contents/interfaces.html#oml.interfaces.metrics.IBasicMetric


IBasic Metric: 메트릭 계산

```
# https://lightning.ai/docs/pytorch/stable/extensions/generated/lightning.pytorch.loggers.TensorBoardLogger.html
# 1) Logging with Tensorboard
logger = TensorBoardLogger(".")

# 2) Logging with Neptune
# https://lightning.ai/docs/pytorch/stable/extensions/generated/lightning.pytorch.loggers.NeptuneLogger.html
logger = NeptuneLogger(api_key="", project="", log_model_checkpoints=False)

# 3) Logging with Weights and Biases
# https://lightning.ai/docs/pytorch/stable/extensions/generated/lightning.pytorch.loggers.WandbLogger.html
import os
os.environ["WANDB_API_KEY"] = ""
logger = WandbLogger(project="test_project", log_model=False)
```

>>> PyTorch Lightning 모델

```
# https://open-metric-learning.readthedocs.io/en/latest/contents/lightning.html#extractormodule
pl_model = ExtractorModule(model, criterion, optimizer)
```

ExtractorModule(): 라이트닝으로 모델을 훈련시키기 위한 기본 모듈

- 하이퍼파라미터 의미 

model, "vits16_dino"라는 이름의 ViT 모델사용.

criterion, 주어진 샘플의 임베딩을 잘 구분하도록 학습하는 데 도움이되는 손실함수 TripletLossWithMiner 사용.

optimizer, SGD (Stochastic Gradient Descent)사용.


>>>trainer 초기화 pl.Trainer

```
# https://lightning.ai/docs/pytorch/stable/common/trainer.html
trainer = pl.Trainer(max_epochs=3, callbacks=[metric_callback], num_sanity_val_steps=0, logger=logger)
```

pl.Trainer는 PyTorch Lightning 라이브러리에서 제공하는 훈련 관리자.

max_epochs=3 최대 에포크 3설정. 

callbacks=[metric_callback], 콜백은 훈련 과정 중에 특정 이벤트가 발생할 때 호출되는 사용자 지정 함수.
이 함수에서 콜백은 모델의 성능 메트릭을 계산하고 기록한다.

num_sanity_val_steps=0, 초기 검증 단계에서 실행할 검증 단계 수를 설정하는 매개변수.

매개변수를 0으로 설정하면 초기 검증 단계를 건너뛴다.

logger=logger, 훈련 과정 중에 사용할 로거를 설정
훈련 및 검증 메트릭을 기록하고 시각화하기 위해 로그 기록.

#모델 훈련

```
trainer.fit(pl_model, train_dataloaders=train_loader,val_dataloaders=val_loader)
```

fit 메서드를 호출하여 모델을 훈련. 다음의 인자들을 받는다.

pl_model, LightningModule 클래스를 상속.

train_dataloaders=train_loader,  훈련 데이터를 제공하는 데이터로더 모델이 학습에 사용할 미니배치를 생성.

val_dataloaders=val_loader, 검증 데이터를 제공하는 데이터로더 모델이 훈련 중에 성능을 평가하는 데 사용.

>> [Using a trained model for retrieval](https://colab.research.google.com/drive/1S2nK6KaReDm-RjjdojdId6CakhhSyvfA?usp=share_link)
```
df_val["path"] = df_val["path"].apply(lambda x: MOCK_DATASET_PATH / x)
```
![image](https://github.com/Suyeon-j/study_oml/assets/66247203/c03ed1bb-686d-4937-a577-0e90289c4e3a)

```
queries = df_val[df_val["is_query"]]["path"].tolist()
```
![image](https://github.com/Suyeon-j/study_oml/assets/66247203/2a932bd0-ec9a-41fe-bad4-ade6ae8a643c)

```
galleries = df_val[df_val["is_gallery"]]["path"].tolist()
```
![image](https://github.com/Suyeon-j/study_oml/assets/66247203/b497d6fa-113d-4317-9c50-10282cef0031)

```
transform, _ = get_transforms_for_pretrained("vits16_dino")
```
![image](https://github.com/Suyeon-j/study_oml/assets/66247203/43752b2a-8d65-498e-af81-15b012622567)

```
args = {"num_workers": 0, "batch_size": 8}
features_queries = inference_on_images(model, paths=queries, transform=transform, **args)
```
![image](https://github.com/Suyeon-j/study_oml/assets/66247203/f3e978ef-9f5e-456d-bc7e-675404e5c73c)

```
features_galleries = inference_on_images(model, paths=galleries, transform=transform, **args)
```
![image](https://github.com/Suyeon-j/study_oml/assets/66247203/b926c703-e9e5-445a-b3c6-4e49dfc6c4cd)

```
# https://open-metric-learning.readthedocs.io/en/latest/_modules/oml/utils/misc_torch.html
dist_mat = pairwise_dist(x1=features_queries, x2=features_galleries)
```
return: [N, M] 모양의 쌍방향 거리
![image](https://github.com/Suyeon-j/study_oml/assets/66247203/275f7f80-a511-4b18-ae1d-33f701b75402)

```
# https://pytorch.org/docs/stable/generated/torch.argmin.html
ii_closest = torch.argmin(dist_mat, dim=1)
```
![image](https://github.com/Suyeon-j/study_oml/assets/66247203/48dbf45e-19c3-41ab-bfff-a8033e975127)
